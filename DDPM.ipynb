{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1341d9-0d4f-4396-aa0f-81329d8460d8",
   "metadata": {},
   "source": [
    "# Deep Learning: Image Generation using DDPM\n",
    "\n",
    "Authors: Jakub Borek, Bartosz Dybowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224f600-34dc-4b30-83e7-520b3275188b",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ae3e8-c16e-48b3-b755-56a9bb8b4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision diffusers accelerate scipy scikit-image matplotlib imageio kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44ec31-4000-4976-9b12-899d1964315c",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7946217-ce89-48b7-b59f-53cf827052da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import inception_v3\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe3de3-44c2-4767-a7be-87f16a4e57ad",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ad508-408e-49ed-bd44-81c79013a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_choice = \"cats\"  # change to \"cats\" or \"cats_and_dogs\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Model parameters\n",
    "image_size = 64\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "early_stopping_patience = 3\n",
    "save_dir = \"ddpm_outputs_cats\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(save_dir, \"ddpm_model.pt\")\n",
    "fid_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdb0fb-14d2-42bc-9e85-b46de19ef966",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838f04e-3475-4923-a0f7-683997e9793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380944b-b737-4c3f-bdbd-6f0ad511679f",
   "metadata": {},
   "source": [
    "## Choose dataset and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48306318-c75f-461d-a70c-92862e9120ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kaggle.json\", \"r\") as f:\n",
    "    kaggle_token = json.load(f)\n",
    "\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
    "    json.dump(kaggle_token, f)\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
    "\n",
    "if dataset_choice == \"cats\":\n",
    "    dataset_path = \"./cat-dataset/cats/Data\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        !kaggle datasets download -d borhanitrash/cat-dataset\n",
    "        !unzip -q cat-dataset.zip -d cat-dataset\n",
    "\n",
    "elif dataset_choice == \"cats_and_dogs\":\n",
    "    dataset_path = \"dogs-vs-cats/train\"\n",
    "    if not os.path.exists(dataset_path):\n",
    "        !kaggle competitions download -c dogs-vs-cats\n",
    "        !unzip -q dogs-vs-cats.zip\n",
    "        !unzip -q train.zip -d dogs-vs-cats\n",
    "\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "print(\"Files in dataset:\", len(os.listdir(dataset_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3427a-2590-46bc-8f41-3ca07b065762",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198670a7-2aa6-43cc-93a2-48d306a487ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(image_folder, \"*\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c377347-8916-4c63-908b-6049046cc9bb",
   "metadata": {},
   "source": [
    "## Transform images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cecff-a9ec-414a-b84e-449f60c71be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26953a-f282-4b1e-ba62-5ef79b6596d7",
   "metadata": {},
   "source": [
    "## Choose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b5eef-864b-4a78-9b47-28e7ce62121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ext = {\".jpg\", \".png\", \".jpeg\"}\n",
    "dataset = ImageFolderDataset(dataset_path, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9c04d-9669-46b1-b442-ee61ccc9353b",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008752f7-c782-46ca-843a-d92a39cc0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 256),\n",
    "    down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "    norm_num_groups=32,\n",
    "    act_fn=\"silu\"\n",
    ").to(device)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37704f1a-e543-44fb-b288-4eb4a449c029",
   "metadata": {},
   "source": [
    "## Define image generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca059b5-c23d-44ca-a447-a53a31df54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, n_samples=100, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    latents = torch.randn((n_samples, 3, image_size, image_size)).to(device)\n",
    "    for t in reversed(range(noise_scheduler.config.num_train_timesteps)):\n",
    "        ts = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(latents, ts).sample\n",
    "        latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    images = (latents.clamp(-1, 1) + 1) / 2\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16936b1-ab2b-4706-b6cc-f5a2fb4f562c",
   "metadata": {},
   "source": [
    "## Define FID calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e0d91-e17d-4ac2-b304-949161bda55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(real_imgs, fake_imgs):\n",
    "    inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception.eval()\n",
    "    def get_activations(imgs):\n",
    "        with torch.no_grad():\n",
    "            resized = nn.functional.interpolate(imgs, size=(299, 299), mode='bilinear')\n",
    "            preds = inception(resized).detach().cpu().numpy()\n",
    "        return preds\n",
    "    real_act = get_activations(real_imgs)\n",
    "    fake_act = get_activations(fake_imgs)\n",
    "\n",
    "    mu1, sigma1 = real_act.mean(axis=0), np.cov(real_act, rowvar=False)\n",
    "    mu2, sigma2 = fake_act.mean(axis=0), np.cov(fake_act, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2)).real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f1a72-5752-4307-a932-f6e657197a9e",
   "metadata": {},
   "source": [
    "## Define interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11166acf-a22c-464a-aac4-e9131b5849bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_generate(model, z1, z2, steps=8):\n",
    "    model.eval()\n",
    "    latents = [z1 * (1 - alpha) + z2 * alpha for alpha in torch.linspace(0, 1, steps + 2)]\n",
    "    imgs = []\n",
    "    with torch.no_grad():\n",
    "        for z in latents:\n",
    "            x = z.unsqueeze(0).to(device)\n",
    "            for t in reversed(range(noise_scheduler.config.num_train_timesteps)):\n",
    "                ts = torch.tensor([t], device=device)\n",
    "                pred = model(x, ts).sample\n",
    "                x = noise_scheduler.step(pred, t, x).prev_sample\n",
    "            imgs.append((x.clamp(-1, 1) + 1) / 2)\n",
    "    return torch.cat(imgs), imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b31062-b636-44c4-a3be-28a883c203e3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee1ff2-606a-407a-b222-cf9f64e02a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fid = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "loss_log = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    best_loss, worst_loss = float('inf'), -float('inf')\n",
    "    best_img, worst_img = None, None\n",
    "    epoch_loss_total = 0\n",
    "    epoch_batches = 0\n",
    "\n",
    "    for x in loop:\n",
    "        x = x.to(device)\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(0, noise_scheduler.num_train_timesteps, (x.size(0),), device=device).long()\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise, t)\n",
    "\n",
    "        pred = model(noisy_x, t).sample\n",
    "        loss = nn.MSELoss()(pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss_total += batch_loss\n",
    "            epoch_batches += 1\n",
    "\n",
    "            if batch_loss < best_loss:\n",
    "                best_loss = batch_loss\n",
    "                best_img = x\n",
    "            if batch_loss > worst_loss:\n",
    "                worst_loss = batch_loss\n",
    "                worst_img = x\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Mean loss\n",
    "    epoch_avg_loss = epoch_loss_total / epoch_batches\n",
    "    loss_log.append(epoch_avg_loss)\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "    # Save best/worst image examples\n",
    "    if best_img is not None:\n",
    "        save_image((best_img + 1) / 2, os.path.join(save_dir, f\"best_epoch{epoch+1}.png\"))\n",
    "    if worst_img is not None:\n",
    "        save_image((worst_img + 1) / 2, os.path.join(save_dir, f\"worst_epoch{epoch+1}.png\"))\n",
    "\n",
    "    # Evaluate FID and save\n",
    "    real_batch = next(iter(dataloader))[:128].to(device)\n",
    "    fake_batch = generate_images(model, n_samples=128, seed=epoch)\n",
    "    fid_score = calculate_fid(real_batch, fake_batch)\n",
    "    fid_log.append(fid_score)\n",
    "    print(f\"FID Epoch {epoch+1}: {fid_score:.2f}\")\n",
    "\n",
    "    save_image((real_batch[:100] + 1) / 2, os.path.join(save_dir, f\"real_batch_epoch{epoch+1}.png\"), nrow=10)\n",
    "    save_image((fake_batch[:100] + 1) / 2, os.path.join(save_dir, f\"fake_batch_epoch{epoch+1}.png\"), nrow=10)\n",
    "\n",
    "    # Early stopping\n",
    "    if fid_score < best_fid:\n",
    "        best_fid = fid_score\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"ddpm_best_fid.pt\"))\n",
    "        print(\"Best FID improved, model saved.\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No FID improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35c132-564a-4e7d-8be1-7668858e6b1d",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd9841-d76f-430d-8d88-3e62fbf1c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE INTERPOLATION ===\n",
    "z1 = torch.randn((3, image_size, image_size)).to(device)\n",
    "z2 = torch.randn((3, image_size, image_size)).to(device)\n",
    "interpolated, interpolated_list = interpolate_and_generate(model, z1, z2, steps=8)\n",
    "save_image(interpolated, os.path.join(save_dir, \"interpolation_final.png\"), nrow=5)\n",
    "print(\"Interpolation image saved.\")\n",
    "\n",
    "# === SAVE INTERPOLATION AS GIF ===\n",
    "imgs = [img.squeeze().permute(1, 2, 0).cpu().numpy() for img in interpolated_list]\n",
    "imgs = [(np.clip(img * 255, 0, 255)).astype(np.uint8) for img in imgs]\n",
    "imageio.mimsave(os.path.join(save_dir, \"interpolation.gif\"), imgs, duration=0.4)\n",
    "print(\"Interpolation GIF saved.\")\n",
    "\n",
    "# === SAVE FID LOG ===\n",
    "fid_txt = os.path.join(save_dir, \"fid_scores.txt\")\n",
    "with open(fid_txt, \"w\") as f:\n",
    "    for i, score in enumerate(fid_log):\n",
    "        f.write(f\"Epoch {i+1}: FID = {score:.4f}\\n\")\n",
    "\n",
    "# === PLOT FID ===\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(fid_log)+1), fid_log)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"FID Score\")\n",
    "plt.title(\"FID over Epochs\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(save_dir, \"fid_plot.png\"))\n",
    "plt.close()\n",
    "print(\"FID plot saved.\")\n",
    "\n",
    "# === SAVE LOSS LOG ===\n",
    "loss_txt = os.path.join(save_dir, \"loss_log.txt\")\n",
    "with open(loss_txt, \"w\") as f:\n",
    "    for i, l in enumerate(loss_log):\n",
    "        f.write(f\"Epoch {i+1}: Loss = {l:.6f}\\n\")\n",
    "\n",
    "# === PLOT LOSS ===\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(loss_log)+1), loss_log, color='orange')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(save_dir, \"loss_plot.png\"))\n",
    "plt.close()\n",
    "print(\"Loss plot saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
